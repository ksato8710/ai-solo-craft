# Article Quality Assessment — 記事品質評価スキル

## 概要

**目的:** 最高品質のキュレーション記事を継続的に生み出すための、厳密な評価システム。

このスキルは **使うたびに自己拡張** する設計:
- 評価結果を蓄積し、パターンを学習
- 高評価・低評価の特徴を記録
- 評価基準自体も進化させる

> **ゴール:** 玉石混交のAI記事の中で、**このメディアのキュレーションは信頼できる** と認識されること。

---

## 評価フレームワーク: AQS（Article Quality Score）

5つの軸 × 20点 = 100点満点

| 軸 | 評価観点 | 配点 |
|----|---------|------|
| **1. ソース品質 (Source Quality)** | 一次情報への到達度、出典の信頼性 | 0-20 |
| **2. 情報密度 (Information Density)** | 無駄のない構成、定量データの充実度 | 0-20 |
| **3. 独自価値 (Unique Value)** | 他では得られない視点・分析・文脈 | 0-20 |
| **4. 読者適合 (Reader Fit)** | ソロビルダーにとっての即時有用性 | 0-20 |
| **5. 構造完成度 (Structure Completeness)** | タイトル・構成・リンク整合性 | 0-20 |

### ティア判定

| スコア | ティア | 判定 |
|--------|--------|------|
| 90-100 | S | 傑出: 即公開 + SNS積極展開 |
| 80-89 | A | 優良: 即公開可 |
| 70-79 | B | 良好: 軽微な改善後に公開 |
| 60-69 | C | 要改善: 修正必須 |
| 0-59 | D | 不可: 大幅リライトまたは破棄 |

---

## 評価基準の詳細

### 1. ソース品質 (Source Quality) — 0-20点

**最高品質のキュレーションは、最高品質のソースから始まる。**

| 点数 | 基準 |
|------|------|
| 18-20 | 公式発表（プレスリリース/公式ブログ）を直接引用 + 補足ソース2つ以上 |
| 15-17 | 公式発表あり + 補足ソース1つ |
| 12-14 | 信頼性の高い二次ソース（TechCrunch, The Verge等）のみ |
| 8-11 | 複数ソースはあるが、一次情報への到達が不十分 |
| 4-7 | 単一の二次ソースのみ |
| 0-3 | ソース不明/未確認/X投稿のみ |

**減点チェック:**
- [ ] 「検討中」と「正式発表」の混同 → -5
- [ ] 発表日付の未特定 → -3
- [ ] 類似サービス名の混同（例: FigJam vs Figma） → -5

### 2. 情報密度 (Information Density) — 0-20点

**すべての文に意味がある。無駄な文は1つもない。**

| 点数 | 基準 |
|------|------|
| 18-20 | 定量データ3つ以上 + 全文が価値を持つ + 読み飛ばせる部分がない |
| 15-17 | 定量データ2つ + ほぼ無駄がない |
| 12-14 | 定量データ1つ + 一部冗長 |
| 8-11 | 定量データなし or 冗長な部分が目立つ |
| 4-7 | 実質的に価値ある情報が少ない |
| 0-3 | 空虚な言い回しが多い |

**定量データの例:**
- 価格・コスト（「$20/月」「$0.04/枚」）
- 性能指標（「1Mトークン対応」「77.1%達成」）
- 時間（「24時間以内」「2月6日に正式発表」）
- 規模（「10万ユーザー」「ARR $1M」）

### 3. 独自価値 (Unique Value) — 0-20点

**なぜこの記事を読むべきか。他との差別化。**

| 点数 | 基準 |
|------|------|
| 18-20 | 独自の分析・考察 + 読者への示唆 + 他では得られない視点 |
| 15-17 | ソロビルダー視点での解釈・文脈付け |
| 12-14 | 情報の整理・要約は丁寧だが、独自分析は薄い |
| 8-11 | ほぼ転載・翻訳に近い |
| 4-7 | 他メディアと差がない |
| 0-3 | コピペ/全文翻訳 |

**独自価値の例:**
- 「ソロビルダーにとって何が変わるか」
- 「これを活用するための具体的アクション」
- 「競合/代替手段との比較」
- 「日本市場での意味」

### 4. 読者適合 (Reader Fit) — 0-20点

**ターゲット読者（AIソロビルダー）にとっての有用性。**

| 点数 | 基準 |
|------|------|
| 18-20 | 読者が「今日から使える」情報 + 明確なアクションポイント |
| 15-17 | 読者の意思決定に直接影響する情報 |
| 12-14 | 関連はあるが、即時活用は難しい |
| 8-11 | ソロビルダーには関係が薄い |
| 4-7 | エンタープライズ向け/研究者向けに偏っている |
| 0-3 | ターゲット不明確 |

**読者適合チェック:**
- [ ] 「個人開発者が使えるか」の視点があるか
- [ ] 料金・コストへの言及があるか
- [ ] 導入の難易度への言及があるか
- [ ] 既存ツールからの移行コストへの言及があるか

### 5. 構造完成度 (Structure Completeness) — 0-20点

**プロフェッショナルな仕上がり。**

| 点数 | 基準 |
|------|------|
| 18-20 | タイトル・リード・本文・CTA が完璧に連動 + 内部リンク整合 |
| 15-17 | 構成は良いが、軽微な改善余地あり |
| 12-14 | 基本構成はOKだが、リンク不足やタイトル改善の余地 |
| 8-11 | 構成に問題あり（リード不足、見出し不整合等） |
| 4-7 | 構成が崩れている |
| 0-3 | 構成として成立していない |

**構造チェック:**
- [ ] タイトルが「何が起きたか」を伝えているか
- [ ] リード文（最初の1-2文）で全体像が掴めるか
- [ ] 関連プロダクトへの `/products/[slug]` リンクがあるか
- [ ] 出典リンクが本文中に配置されているか
- [ ] frontmatter（contentType, tags, relatedProducts）が正確か

---

## 評価プロセス

### Step 1: 初期スキャン（2分）

以下を確認:
1. タイトルを読む — 「何が起きたか」が分かるか？
2. リードを読む — 全体像が掴めるか？
3. 出典を確認 — 一次情報があるか？
4. 定量データをカウント — いくつあるか？

### Step 2: 5軸評価（5分）

各軸を評価し、スコアと根拠を記録:

```markdown
## AQS評価結果

| 軸 | スコア | 根拠 |
|----|--------|------|
| ソース品質 | XX/20 | ... |
| 情報密度 | XX/20 | ... |
| 独自価値 | XX/20 | ... |
| 読者適合 | XX/20 | ... |
| 構造完成度 | XX/20 | ... |
| **合計** | **XX/100** | **ティア: X** |
```

### Step 3: 改善ポイントの特定

スコアが低い軸（14点以下）について、具体的な改善アクションを明示:

```markdown
### 改善ポイント
1. **[軸名]**: [具体的な問題] → [改善アクション]
2. ...
```

### Step 4: 判定

| ティア | アクション |
|--------|-----------|
| S/A | 即公開可。記録を評価DBに追加 |
| B | 改善ポイントを修正後、再評価 |
| C/D | 大幅リライトまたは破棄 |

---

## 評価パターンDB（自動拡張）

> **重要ルール**: 評価を実施するたびに、特徴的なパターンを以下のテーブルに追記すること。
> 高評価の成功パターンも、低評価の失敗パターンも、両方記録する。
> スキルは使うたびに賢くなる。

### 高評価パターン（Sティア/Aティア）

| 日付 | 記事slug | AQS | 成功要因 |
|------|---------|-----|---------|
| 2026-02-22 | claude-code-security-vulnerability-scanning | 85 | 公式ブログ直引用 + ソロビルダー向けコスト計算 + 競合比較 |
| _(評価実施時に追記)_ | | | |

### 低評価パターン（Cティア以下）

| 日付 | 記事slug | AQS | 失敗要因 | 教訓 |
|------|---------|-----|---------|------|
| 2026-02-18 | x-api-pay-per-use | 52 | 「検討中」と「正式発表」の混同、発表日未特定 | 料金系は公式発表日を必ず確認 |
| 2026-02-18 | figjam-claude-integration | 55 | FigJamとFigmaの混同、誤った関連事例 | 類似サービス名は別物として扱う |
| _(評価実施時に追記)_ | | | | |

### 頻出改善パターン

| パターン | 発生頻度 | 対策 |
|---------|---------|------|
| 一次ソース未到達 | 高 | 必ず公式発表・プレスリリースを探す |
| 定量データ不足 | 中 | 価格・性能・時期を最低1つ入れる |
| ソロビルダー視点の欠如 | 中 | 「個人開発者が使えるか」を明示 |
| _(評価実施時に追記)_ | | |

---

## キャリブレーションDB（予測 vs 実績）

> 評価スコアと実際のパフォーマンス（PV、SNS反応）を記録し、
> 評価基準の精度を継続的に向上させる。

### 記録フォーマット

```markdown
| 日付 | slug | AQS予測 | PV(7日) | X反応 | 乖離分析 |
|------|------|---------|---------|-------|---------|
| 2026-02-XX | xxx | 85 | 1200 | 15RT | 予測通り/過大評価/過小評価 + 理由 |
```

### キャリブレーション結果

| 期間 | 平均乖離 | 傾向 | 調整アクション |
|------|---------|------|---------------|
| _(週次で振り返り時に追記)_ | | | |

---

## ワークフロー統合

### 記事作成フロー内での位置づけ

```
Phase 1: news-research（収集）
Phase 2: news-evaluation（NVA選定）
Phase 3: digest-writer / individual-article（執筆）
Phase 3.5: thumbnail-generator（サムネイル）
Phase 4: article-quality-assessment（品質評価）← THIS
Phase 5: publish-gate（公開チェック）
Phase 6: x-publisher（SNS告知）
```

### 評価タイミング

1. **公開前必須評価**: Phase 4で全記事を評価
2. **公開後振り返り**: 週次でキャリブレーションDBを更新
3. **パターン学習**: 月次で評価パターンDBをレビュー

---

## 参照ドキュメント

| ファイル | 内容 |
|---------|------|
| `.claude/skills/editorial-standards.md` | 編集基準（タイトルルール等） |
| `.claude/skills/news-curation.md` | キュレーション手順 |
| `docs/operations/CHECKLIST.md` | 公開前チェックリスト |
| `docs/operations/EDITORIAL.md` | タイトルルール詳細 |

---

## 評価実施時の必須アクション

1. **評価結果を記録**: 上記フォーマットで記録
2. **パターンDBを更新**: S/A評価またはC/D評価の場合、成功/失敗要因を追記
3. **改善指示を出す**: B以下の場合、具体的な改善アクションを明示
4. **再評価**: 修正後に再度評価し、ティアが上がったことを確認

---

*最高品質のキュレーションは、最高品質の評価から生まれる。*
*このスキルは使うたびに賢くなる。評価を重ねるほど、基準は研ぎ澄まされる。* 🥊
